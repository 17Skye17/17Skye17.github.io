---
title: 'Why Pytorch Distributed Data Parallel?'
date: 2021-04-07
permalink: /posts/2012/08/blog-post-26/
tags:
  - pytorch
  - DDP
  - SyncBN
---

Pytorch DDP (Distributed Data Parallel) v.s. DP (Data Parallel)

区别：
1. 对于一般不是特别吃计算资源的实验来说，主要选择DDP的原因在于它能做到syncbn，DP的bn是在单卡上计算running mean和running variance而不在多卡上同步，不能准确估计数据分布。
 
2. 对于很吃计算资源的实验，DDP更快因为进程间通信采用了比较高效的ring-reduce机制来交换进程间梯度。
