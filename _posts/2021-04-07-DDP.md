---
title: 'Why Pytorch Distributed Data Parallel?'
date: 2021-04-07
permalink: /posts/2012/08/blog-post-26/
tags:
  - pytorch
  - DDP
  - SyncBN
---

Pytorch DDP (Distributed Data Parallel) v.s. DP (Data Parallel)

区别：
1. 对于一般不是特别吃计算资源的实验来说，主要选择DDP的原因在于它能做到syncbn，DP的bn是在单卡上计算running mean和running variance而不在多卡上同步，不能准确估计数据分布。syncbn简单来说就是将每个process中的数据计算running mean和running variance，然后通过process间通信（all_gather）将所有process的running mean和running variance同步，得到总的值。（数学上可证明数据划分计算的结果与总体结果一致）
 
2. 对于很吃计算资源的实验，DDP更快因为进程间通信采用了比较高效的ring-reduce机制来交换进程间梯度。


一些需要注意的点：

1. DDP是每个process启动一个main函数，所以在程序中经常会print的内容会重复number of processes次，一般可以通过distributed.get_rank()来指定一个process来print。

2. dataloader的数据在多个process间分发是通过sampler完成的，因此不论是train还是test都需要将原本的batchsize变成batchsize//num_processes。（一般单机多卡的情况下，num_processes==num_gpus）。

3. 随机数种子设置：一般情况下，特别是如果有数据增强操作，要注意给每个rank都set seed。

4. train的时候loss值在每个process上正常更新，但如果要print出来或者另作他用需要all_reduce来返回总体的loss； test时的prediction和label一般需要all_gather起来去做evaluation。

5. 最后run的时候，通过python -m torch.distributed.launch --nproc_per_node num_gpus来启动多个process。

To be continued...
